{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Private 1위] Model Ensembling\n",
    "두 가지 접근 방법으로 얻은 모델 결과를 종합했습니다.\n",
    "\n",
    "a. Text classification models (e.g., bert)\n",
    "- class imbalance 문제 해결하기 위해 first party와 second party의 순서를 바꾸는 data augmentation 방법 적용.\n",
    "- pretrained weights 부분은 모두 freeze 한 뒤 추가되는 residual connection이 적용된 네트워크 직접 설계.\n",
    "- 텍스트 분류 모델 중 4가지 모델 활용.\n",
    "\n",
    "b. Large language models (e.g., vicuna-13b)\n",
    "- 예측해야 하는 테스트 데이터와 유사한 판결 사례들을 함께 활용하여 예측하는 few-shot learning 접근 방법 활용.\n",
    "- ALBERT 기반 판결 내용을 하나의 embedding vector로 압축하여 유사한 판결 사례들을 cosine similarity 계산하여 선별.\n",
    "- 언어 모델 중 Vicuna-13B 활용.\n",
    "\n",
    "- 결론: Private score 0.57258 달성."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reproducibility\n",
    "- 하드웨어 스펙: A6000 x 1, RAM 252GB\n",
    "\n",
    "- 공정한 검정을 위해 사전 학습된 모델 파일들은 Google Drive 링크로 참조하시면 됩니다. (https://drive.google.com/file/d/1B_litWreHZnkRN4VZrOCczbgePl4Szxb/view?usp=sharing)\n",
    "- 하나의 코드 파일로 정리할 수 있지만, 너무 길어질 것 같아 github으로 정리했습니다. (https://github.com/shjo-april/DACON_Judgement_of_Court_1st_Solution)\n",
    "- 핵심 코드 및 실행 방법은 아래 순서 참조하시면 됩니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫번째, text classification models 관련 환경 설정 및 테스트 방법입니다.\n",
    "- venv 활용하였고, 명령어는 아래와 같습니다.\n",
    "```bash\n",
    "python3 -m venv venv\n",
    "source ./venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- 실행 과정입니다.\n",
    "```bash\n",
    "# 학습 및 테스트 데이터 전처리\n",
    "python3 preprocess.py\n",
    "\n",
    "# 학습 데이터에 대한 embedding vectors 추출\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_embs_for_trainval.py --model google/bigbird-pegasus-large-bigpatent --tag bigbird-pegasus-large-bigpatent\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_embs_for_trainval.py --model google/rembert --tag rembert\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_embs_for_trainval.py --model microsoft/deberta-v2-xxlarge --tag deberta-v2-xxlarge\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_embs_for_trainval.py --model albert-xxlarge-v2 --tag albert-xxlarge-v2\n",
    "\n",
    "# 테스트 데이터에 대한 embedding vectors 추출\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_embs_for_test.py --model google/bigbird-pegasus-large-bigpatent --tag bigbird-pegasus-large-bigpatent\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_embs_for_test.py --model google/rembert --tag rembert\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_embs_for_test.py --model microsoft/deberta-v2-xxlarge --tag deberta-v2-xxlarge\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_embs_for_test.py --model albert-xxlarge-v2 --tag albert-xxlarge-v2\n",
    "\n",
    "# 언어 모델용 데이터 전처리\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_embs_for_llm.py --file ./open/train.json\n",
    "CUDA_VISIBLE_DEVICES=1 python3 extract_embs_for_llm.py --file ./open/test.json\n",
    "python3 generate_qa_list_for_llm.py # for few-shot learning\n",
    "\n",
    "# 사전 학습된 모델 기반 테스트 샘플 별 예측 결과 생성\n",
    "CUDA_VISIBLE_DEVICES=0 python3 infer_classification_models.py \\\n",
    "--model_names rembert,albert-xxlarge-v2,deberta-v2-xxlarge,bigbird-pegasus-large-bigpatent\n",
    "```\n",
    "\n",
    "- 두번째, large language models 관련 환경 설정 및 테스트 방법입니다.\n",
    "```bash\n",
    "deactivate\n",
    "\n",
    "cd llm\n",
    "\n",
    "python3 -m venv venv\n",
    "source ./venv/bin/activate\n",
    "\n",
    "pip3 install --upgrade pip\n",
    "pip3 install -e .\n",
    "\n",
    "git lfs install\n",
    "git clone https://huggingface.co/lmsys/vicuna-13b-v1.3\n",
    "\n",
    "python3 -m fastchat.serve.controller\n",
    "python3 -m fastchat.serve.model_worker --model-path vicuna-13b-v1.3 --port 21002\n",
    "\n",
    "# Vicuna-13B 실행하여 테스트 셋에 대한 예측 결과 생성\n",
    "python3 -m run_llm \\\n",
    "--controller-address \"http://localhost:21001\" --model-name vicuna-13b-v1.3 \\\n",
    "--temperature 0.001 --max-new-tokens 100\n",
    "```\n",
    "\n",
    "- 세번째, 각 단계 별 얻은 결과 종합하여 최종 예측 결과 생성합니다.\n",
    "```bash\n",
    "python3 ensemble_all_results.py\n",
    "```\n",
    "\n",
    "- 마지막, 제출 당시 활용한 결과와 같은지 검증합니다.\n",
    "```bash\n",
    "# 출력 결과: 공식 결과와 100% 일치합니다.\n",
    "python3 quantify_reproducibility.py\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training\n",
    "- 총 4가지 모델 별 학습.\n",
    "- 핵심 코드는 아래 추가 코드 블록 생성하여 설명. 총 4가지 핵심 정리.\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 python3 train.py --model bigbird-pegasus-large-bigpatent\n",
    "CUDA_VISIBLE_DEVICES=0 python3 train.py --model rembert\n",
    "CUDA_VISIBLE_DEVICES=0 python3 train.py --model deberta-v2-xxlarge\n",
    "CUDA_VISIBLE_DEVICES=0 python3 train.py --model albert-xxlarge-v2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [핵심 1] first party와 second party를 서로 바꿔서 class imbalance 문제 완화 및 data augmentation.\n",
    "... (생략)\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, path, domain):\n",
    "        target_data = pickle.load(open(path, 'rb'))[domain]\n",
    "\n",
    "        self.pos_data = []\n",
    "        self.neg_data = []\n",
    "        \n",
    "        for data in target_data:\n",
    "            first_emb = data['first_party']\n",
    "            second_emb = data['second_party']\n",
    "            fact_emb = data['facts']\n",
    "\n",
    "            if len(first_emb.shape) > 1:\n",
    "                first_emb = first_emb[0]\n",
    "                second_emb = second_emb[0]\n",
    "                fact_emb = fact_emb[0]\n",
    "            \n",
    "            # [핵심 1]\n",
    "            if data['output'] == 'Victory':\n",
    "                self.pos_data.append([first_emb, second_emb, fact_emb, 0])\n",
    "                self.neg_data.append([second_emb, first_emb, fact_emb, 1])\n",
    "            else:\n",
    "                self.pos_data.append([second_emb, first_emb, fact_emb, 0])\n",
    "                self.neg_data.append([first_emb, second_emb, fact_emb, 1])\n",
    "\n",
    "        self.dataset = self.pos_data + self.neg_data\n",
    "\n",
    "... (생략)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [핵심 2] Backbone 자체를 freeze하여 최소한의 네트워크만 학습하기 때문에 residual connection 기반 multilayer perceptron 디자인.\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Residual_MLP(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()        \n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "        self.norm1 = nn.BatchNorm1d(out_features)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.do1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(out_features, out_features)\n",
    "        self.norm2 = nn.BatchNorm1d(out_features)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.do2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc3 = nn.Linear(out_features, in_features)\n",
    "        self.norm3 = nn.BatchNorm1d(in_features)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.do3 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ = self.fc1(x)\n",
    "        x_ = self.norm1(x_)\n",
    "        x_ = self.act1(x_)\n",
    "        x_ = self.do1(x_)\n",
    "\n",
    "        x_ = self.fc2(x_)\n",
    "        x_ = self.norm2(x_)\n",
    "        x_ = self.act2(x_)\n",
    "        x_ = self.do2(x_)\n",
    "\n",
    "        x_ = self.fc3(x_)\n",
    "        x_ = self.norm3(x_)\n",
    "        x_ = self.act3(x + x_)\n",
    "        return self.do3(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [핵심 3] first party, second party, facts 각각 embedding vectors를 뽑은 뒤 cosine similarity 기반 승자일 수록 높은 similarity를 갖도록 네트워크 설계.\n",
    "... (생략)\n",
    "\n",
    "class CosClassifier(nn.Module):\n",
    "    def __init__(self, num_features, mlp_type):\n",
    "        super().__init__()\n",
    "\n",
    "        if mlp_type == 'residual':\n",
    "            self.mlp_for_person = nn.Sequential(\n",
    "                Residual_MLP(num_features, num_features // 4),\n",
    "                nn.Linear(num_features, num_features // 4)\n",
    "            )\n",
    "            self.mlp_for_facts = nn.Sequential(\n",
    "                Residual_MLP(num_features, num_features // 4),\n",
    "                nn.Linear(num_features, num_features // 4)\n",
    "            )\n",
    "\n",
    "        self.scale = nn.Parameter(torch.Tensor(1)) \n",
    "    \n",
    "    def forward(self, first_embs, second_embs, fact_embs):\n",
    "        first_embs = self.mlp_for_person(first_embs)\n",
    "        second_embs = self.mlp_for_person(second_embs)\n",
    "        fact_embs = self.mlp_for_facts(fact_embs)\n",
    "\n",
    "        # [핵심 3]\n",
    "        first_logits = self.scale * F.cosine_similarity(\n",
    "            F.normalize(first_embs, dim=1),\n",
    "            F.normalize(fact_embs, dim=1),\n",
    "            dim=1\n",
    "        )\n",
    "        second_logits = self.scale * F.cosine_similarity(\n",
    "            F.normalize(second_embs, dim=1),\n",
    "            F.normalize(fact_embs, dim=1),\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        logits = torch.cat([first_logits.unsqueeze(1), second_logits.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [핵심 4] 모델의 uncertainty로 인한 잘못된 예측을 줄이기 위해 uncertainty를 계산한 뒤 해당 정보를 함께 활용하여 ensemble.\n",
    "... (생략)\n",
    "\n",
    "for i in tqdm.tqdm(range(len(test_datasets[0]))):\n",
    "    test_ids = []\n",
    "    preds = []\n",
    "    probs = []\n",
    "    uncertainties = []\n",
    "\n",
    "    for test_dataset, model, name in zip(test_datasets, models, args.model_names.split(',')):\n",
    "        test_id, first, second, fact = test_dataset[i]\n",
    "\n",
    "        first = torch.from_numpy(first).cuda().unsqueeze(0)\n",
    "        second = torch.from_numpy(second).cuda().unsqueeze(0)\n",
    "        fact = torch.from_numpy(fact).cuda().unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prob = torch.softmax(model(first, second, fact)[0], dim=0)\n",
    "            prob = prob.cpu().detach().numpy()\n",
    "\n",
    "        # [핵심 4]\n",
    "        uncertainty = np.sum([p * (1. - p) for p in prob])\n",
    "\n",
    "        test_ids.append(test_id)\n",
    "        preds.append(np.argmax(prob))\n",
    "        probs.append(prob)\n",
    "        uncertainties.append(uncertainty)\n",
    "    \n",
    "    #  [핵심 4]\n",
    "    prob = np.mean([uncertainties[i]*probs[i] for i in [1, 2, 3, 4]], axis=0)\n",
    "\n",
    "... (생략)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
